# Test Strategy & Organization

## Testing Philosophy

`site2rag` follows a **comprehensive testing strategy** with clear separation of concerns:

- **Unit Tests**: Fast, isolated tests for individual components
- **Integration Tests**: End-to-end workflows with real components  
- **Manual Testing**: Real website downloads for validation
- **Consistent Naming**: All test files use kebab-case convention

## Folder Structure

```
tests/
â”œâ”€â”€ unit/                   # Unit tests (fast, isolated)
â”‚   â”œâ”€â”€ services/          # Service-specific unit tests
â”‚   â”‚   â”œâ”€â”€ file-service.test.js
â”‚   â”‚   â”œâ”€â”€ crawl-service.test.js
â”‚   â”‚   â””â”€â”€ *.test.js
â”‚   â”œâ”€â”€ site-processor.test.js
â”‚   â”œâ”€â”€ content-extraction.test.js
â”‚   â””â”€â”€ *.test.js         # Component unit tests
â”œâ”€â”€ integration/           # Integration tests (full workflows)
â”‚   â”œâ”€â”€ crawl-markdown.test.js
â”‚   â”œâ”€â”€ change-detection.test.js
â”‚   â””â”€â”€ *.test.js
â”œâ”€â”€ fixtures/              # Static test data (committed)
â”‚   â”œâ”€â”€ sample.html        # Mock HTML files
â”‚   â”œâ”€â”€ expected-output.md # Reference outputs
â”‚   â””â”€â”€ configs/          # Sample configurations
â”œâ”€â”€ tmp/                   # Temporary outputs (gitignored)
â”‚   â”œâ”€â”€ sites/            # Real website downloads
â”‚   â”‚   â””â”€â”€ example.com/  # Downloaded for testing
â”‚   â”œâ”€â”€ output-*/         # Test output directories
â”‚   â””â”€â”€ *.db             # Test databases
â”œâ”€â”€ testUtils.js           # Shared test utilities
â””â”€â”€ README.md             # This documentation
```

## Naming Conventions

### Test Files
- **Format**: `{component-name}.test.js` (kebab-case)
- **Examples**: 
  - `site-processor.test.js`
  - `file-service.test.js`
  - `crawl-state-service.test.js`

### Test Descriptions
- **Unit tests**: `describe('ComponentName', () => {})`
- **Methods**: `describe('methodName', () => {})`
- **Scenarios**: `it('should do something when condition', () => {})`

### Temporary Files
- **Real sites**: `tests/tmp/sites/{domain}/`
- **Test outputs**: `tests/tmp/output-{test-name}/`
- **Test databases**: `tests/tmp/{test-name}.db`

## Test Categories

### Unit Tests (`tests/unit/`)

**Purpose**: Test individual components in isolation
```javascript
// Example: tests/unit/services/file-service.test.js
describe('FileService', () => {
  describe('saveMarkdown', () => {
    it('should save markdown file with frontmatter', () => {
      // Test implementation
    });
  });
});
```

**Characteristics**:
- âš¡ **Fast execution** (< 100ms per test)
- ğŸ”’ **Isolated** - mock external dependencies
- ğŸ¯ **Focused** - test one function/method
- ğŸ“Š **High coverage** - edge cases and error handling

### Integration Tests (`tests/integration/`)

**Purpose**: Test complete workflows end-to-end
```javascript
// Example: tests/integration/crawl-markdown.test.js
describe('Full Crawl Workflow', () => {
  it('should crawl site and generate markdown', async () => {
    // Test real crawl process
  });
});
```

**Characteristics**:
- ğŸŒ **Real components** - no mocking of internal services
- â±ï¸ **Slower execution** (1-10 seconds per test)
- ğŸ”„ **End-to-end** - full user scenarios
- ğŸ“ **Real data** - may use limited real network requests

### Test Data (`tests/fixtures/`)

**Static test data** committed to repository:
- **Mock HTML**: Realistic website content samples
- **Expected outputs**: Reference markdown files
- **Configurations**: Sample crawl configs
- **Error cases**: Malformed HTML, network errors

## Running Tests

```bash
# All tests (unit + integration)
npm test

# Unit tests only (fast)
npm run test:unit

# Integration tests only
npm run test:integration  

# Specific test file
npm test -- file-service.test.js

# Watch mode for development
npm run test:watch

# Coverage report
npm run test:coverage
```

## Testing Real Websites

### Manual Testing Process
For testing with real websites, use `tests/tmp/sites/`:

```bash
# Download real site for testing
cd tests/tmp/sites
node ../../../bin/site2rag.js example.com --limit 10
```

**Guidelines**:
- âœ… **Use small limits** (--limit 10) to avoid overwhelming servers
- âœ… **Choose stable sites** that won't change frequently  
- âœ… **Document test sites** in test comments
- âœ… **Clean up** old downloads periodically

### Test Site Recommendations
- **Documentation sites**: Usually stable, good structure
- **Personal blogs**: Small, predictable content
- **Educational sites**: Often well-structured HTML

## Development Guidelines

### Writing New Tests

1. **Start with unit tests** for new components
2. **Use descriptive test names** that explain the scenario
3. **Test edge cases** and error conditions
4. **Mock external dependencies** in unit tests
5. **Use real components** in integration tests

### Test Data Management

```javascript
// âœ… Good: Use fixtures for reusable data
const sampleHTML = fs.readFileSync('tests/fixtures/sample.html', 'utf8');

// âŒ Bad: Inline test data in every test
const sampleHTML = '<html><body>...</body></html>';
```

### Temporary File Handling

```javascript
// âœ… Good: Use tests/tmp/ for temporary outputs
const outputDir = 'tests/tmp/output-my-test';

// âŒ Bad: Create temp files in root or src/
const outputDir = './temp-test-output';
```

### Error Testing

```javascript
// âœ… Good: Test both success and failure cases
it('should handle network errors gracefully', async () => {
  // Mock network failure
  // Verify graceful degradation
});
```

## Continuous Integration

Tests run automatically on:
- **Pull Requests**: All tests must pass
- **Main branch commits**: Full test suite + coverage
- **Nightly builds**: Including integration tests with real sites

## Debugging Tests

```bash
# Run single test with verbose output
npm test -- --verbose file-service.test.js

# Debug with inspector
node --inspect-brk node_modules/.bin/vitest run specific.test.js

# View test coverage locally
npm run test:coverage
open coverage/index.html
```

## Best Practices

### âœ… Do
- Use **descriptive test names** that read like specifications
- **Mock external dependencies** in unit tests
- **Clean up** temporary files in test teardown
- **Test edge cases** and error conditions
- **Use fixtures** for reusable test data

### âŒ Don't
- Mix **temporary files** with committed test data
- Use **snake_case** or **camelCase** for test files (use kebab-case)
- Create **large test downloads** that slow CI
- **Hard-code** paths or URLs in tests
- **Skip error cases** - they're often the most important

---

*This testing strategy ensures reliable, maintainable tests that provide confidence in site2rag's functionality across diverse websites and use cases.*